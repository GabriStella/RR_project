---
title:     Time Series Minimum Wage Studies Meta-Analysis
output: 
  beamer_presentation: 
    theme: Warsaw
    colortheme: lily
author:    GS
date:      June, 2022
fontsize:  9pt  
---


```{r, include = F}
load('DataFrame.RData')
library(tidyverse)
library(stargazer)
library(rcompanion)
library(knitr)
library(kableExtra)

id_num = c(seq(1, 18, 1))

implemented = c(rep('No', 15), rep('Yes', 3))

author = c('Kaitz',
           'Mincer',
           'Gramlich',
           'Welch',
           'Ragan',
           'Wachter and Kim',
           'Iden',
           'Ragan',
           'Abowd and Killingsworth',
           'Betsey and Dunson',
           'Brown',
           'Hammermesh',
           'Solon',
           'Wellington',
           'Klerman',
           'Majchrowska and Żółkiewski',
           'China Paper Pag. 10',
           'Bewley et al.')
year = c(
  1970, 1976, 1976, 1976, 1977, 
  1979, 1980, 1981, 1981, 1981,
  1983, 1981, 1985, 1991, 1992,
  2012, 2011, 2015 )

t_stat = c(
  2.30, 2.41, 1.41, 2.22, 1.52,
  2.17, 4.43, 1.70, 1.04, 2.12,
  1.92, 1.63, 2.78, 1.41, 0.45,
  2.15, 1.97, 1.46)

df = c(49, 58, 106, 53, 31, 56, 93, 54, 95, 93, 92, 94, 86, 114, 123, 157, 105, 128)

coef = c(
  0.098, 0.231, 0.094, 0.178, 0.065,
  0.252, 0.226, 0.052, 0.213, 0.139,
  0.096, 0.121, 0.098, 0.066, 0.052,
  0.270, 0.098, 0.072)


log_spec = c(
  0, 0, 1, 1, 1,
  1, 0, 1, 1, 0,
  1, 1, 1, 1, 1,
  1, 1, 1)

no_exp_var = c(10, 5, 17, 6, 8, 11, 10, 9, 8, 10, 11, 5, 17, 17, 5, 8, 6, 10)

autoreg_correction = c(0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1)

dt = data.frame(id_num, implemented, author, year, t_stat, df, coef, log_spec, no_exp_var, autoreg_correction)
dt$error = dt$coef / dt$t_stat
dt$sqrt_df = sqrt(dt$df)
dt$l_sqrt_df = log(dt$sqrt_df)
#save(dt, file = "DataFrame.RData")

#now we're going to reproduce the old model predict in the paper and then we'll calculate the implemented ones.

model_1_old = lm(log(t_stat) ~ l_sqrt_df,
                 data = dt[1:15,])
model_2_old = lm(log(t_stat) ~ l_sqrt_df + autoreg_correction + log_spec,
                 data = dt[1:15,])
model_3_old = lm(log(t_stat) ~ l_sqrt_df + autoreg_correction + log_spec + no_exp_var,
                 data = dt[1:15,])

model_1_new = lm(log(t_stat) ~ l_sqrt_df,
                 data = dt)
model_2_new = lm(log(t_stat) ~ l_sqrt_df + autoreg_correction + log_spec,
                 data = dt)
model_3_new = lm(log(t_stat) ~ l_sqrt_df + autoreg_correction + log_spec + no_exp_var,
                 data = dt)

model_1_old_sub1 = lm(log(t_stat) ~ l_sqrt_df,
                      data = dt[1:15,] %>% filter(year <= 1982))
model_2_old_sub1 = lm(log(t_stat) ~ l_sqrt_df + autoreg_correction + log_spec,
                      data = dt[1:15,] %>% filter(year <= 1982))
model_3_old_sub1 = lm(log(t_stat) ~ l_sqrt_df + autoreg_correction + log_spec + no_exp_var,
                      data = dt[1:15,] %>% filter(year <= 1982))

model_1_old_sub2 = lm(log(t_stat) ~ l_sqrt_df,
                      data = dt[1:15,] %>% filter(id_num != 7))
model_2_old_sub2 = lm(log(t_stat) ~ l_sqrt_df + autoreg_correction + log_spec,
                      data = dt[1:15,] %>% filter(id_num != 7))
model_3_old_sub2 = lm(log(t_stat) ~ l_sqrt_df + autoreg_correction + log_spec + no_exp_var,
                      data = dt[1:15,] %>% filter(id_num != 7))

model_1_new_sub2 = lm(log(t_stat) ~ l_sqrt_df,
                      data = dt %>% filter(id_num != 7))
model_2_new_sub2 = lm(log(t_stat) ~ l_sqrt_df + autoreg_correction + log_spec,
                      data = dt %>% filter(id_num != 7))
model_3_new_sub2 = lm(log(t_stat) ~ l_sqrt_df + autoreg_correction + log_spec + no_exp_var,
                      data = dt %>% filter(id_num != 7))

model_1_old_sub3 = lm(log(t_stat) ~ l_sqrt_df,
                      data = dt[1:15,] %>% filter(log_spec == 1))
model_2_old_sub3 = lm(log(t_stat) ~ l_sqrt_df + autoreg_correction + log_spec,
                      data = dt[1:15,] %>% filter(log_spec == 1))
model_3_old_sub3 = lm(log(t_stat) ~ l_sqrt_df + autoreg_correction + log_spec + no_exp_var,
                      data = dt[1:15,] %>% filter(log_spec == 1))

model_1_new_sub3 = lm(log(t_stat) ~ l_sqrt_df,
                      data = dt %>% filter(log_spec == 1))
model_2_new_sub3 = lm(log(t_stat) ~ l_sqrt_df + autoreg_correction + log_spec,
                      data = dt %>% filter(log_spec == 1))
model_3_new_sub3 = lm(log(t_stat) ~ l_sqrt_df + autoreg_correction + log_spec + no_exp_var,
                      data = dt %>% filter(log_spec == 1))

#2- graphs 

fig_1_old = ggplot(dt[1:15, ], aes(x = sqrt_df, y = t_stat)) +
  geom_point(aes(color = as.factor(implemented))) +
  geom_text(aes(label = id_num), hjust = -0.5, vjust = -0.5) +
  geom_smooth(formula = y ~ x, method = 'lm', color = 'black') +
  labs(#title = 'Figure 1. Estimated t-statistics compared to Degrees of Freedom',
    subtitle = 'We can text here',
    x = 'Square Root of Degrees of Freedom',
    y = 't-statistics (absolute)') +
  theme_bw() + 
  theme(legend.position = 'none')

fig_2_old = ggplot(dt[1:15, ], aes(x = error, y = coef)) +
  geom_point(aes(color = as.factor(implemented))) +
  geom_line(aes(y = 2 * error)) +
  geom_text(aes(label = id_num), hjust = -0.5, vjust = -0.5) +
  geom_smooth(formula = y ~ x, method = 'lm', color = 'black') +
  labs(#title = 'Figure 2. Estimated Employment Elasticity compared to Standard Error Estimate',
    #subtitle = 'Blue colour indicates the studies added to meta-analysis.',
    x = 'Standard Error',
    y = 'Employment Elasticity (absolute)') +
  theme_bw() + 
  theme(legend.position = 'none')

fig_1_new = ggplot(dt, aes(x = sqrt_df, y = t_stat)) +
  geom_point(aes(color = as.factor(implemented))) +
  geom_text(aes(label = id_num), hjust = -0.5, vjust = -0.5) +
  geom_smooth(formula = y ~ x, method = 'lm', color = 'black') +
  labs(#title = 'Figure 1. Estimated t-statistics compared to Degrees of Freedom',
    #subtitle = 'Blue colour indicates the studies added to meta-analysis.',
    x = 'Square Root of Degrees of Freedom',
    y = 't-statistics (absolute)') +
  theme_bw() + 
  theme(legend.position = 'none')

fig_2_new = ggplot(dt, aes(x = error, y = coef)) +
  geom_point(aes(color = as.factor(implemented))) +
  geom_line(aes(y = 2 * error)) +
  geom_text(aes(label = id_num), hjust = -0.5, vjust = -0.5) +
  geom_smooth(formula = y ~ x, method = 'lm', color = 'black') +
  labs(#title = 'Figure 2. Estimated Employment Elasticity compared to Standard Error Estimate',
    #subtitle = 'Blue colour indicates the studies added to meta-analysis.',
    x = 'Standard Error',
    y = 'Employment Elasticity (absolute)') +
  theme_bw() + 
  theme(legend.position = 'none')

#3- tables

tbl1 = stargazer(model_1_old, model_2_old, model_3_old, type = 'latex', digits = 2, header = F,
                 font.size = 'footnotesize', no.space = TRUE)
tbl1 = sub('^.+\\caption.+$','', tbl1)

tbl2 = stargazer(model_1_new, model_2_new, model_3_new, type = 'latex', digits = 2, header = F,
                 font.size = 'footnotesize', no.space = TRUE)
tbl2 = sub('^.+\\caption.+$','', tbl2)
```



## Introduction

- our project is based on the paper written by David Card & Alan B. Krueger in 1995 regarding research of the best-known predictions of standard economic theory that an increase in the minimum wage will lower employment of low-wage workers. Their result indicates that a reduction of between one and three percent in teenage employment as a result of a 10 percent increase in the federal minimum wage. 

- The main idea of our project is to reproduce their research by add our dataframe via Meta-Analysis


-   requirements to re-run the obtained results are:

`R version 4.1.3`, `rmarkdown 2.14`, `tidyverse 1.3.1`, `stargazer 5.2.3`, `rcompanion 2.4.15`

## Meta-Analysis

- A meta-analysis is a statistical analysis that combines the results of multiple scientific studies. Meta-analyses can be performed when there are multiple scientific studies addressing the same question, with each individual study reporting measurements that are expected to have some degree of error. 

- The aim then is to use approaches from statistics to derive a pooled estimate closest to the unknown common truth based on how this error is perceived. Meta-analytic results are considered the most trustworthy source of evidence by the evidence-based medicine literature

## How we can find the data and missing data 

- The original dataframe includes 15 observations of 14 variables by year 1992.  

- consists of the author(s), date of publication (year), t-statistic in absolute terms1 (t_stat), degrees of freedom (df), coefficient of the minimum wage variable in absolute terms2 (coef), number of explanatory variables of the model (no_exp_var), the error of the model (error) and two dummy variables. One binary variable is set to 1, when there was a logarithmic specification of the model (log_spec) and the second one is set to 1, when the autoregression correction was applied (autoreg_correction).


## dt
### t_stat
```{r, echo=FALSE}
#kable(cbind(dt, dt, dt), booktabs = T) %>%
#  kable_styling(latex_options = c("striped", "scale_down"))

summary(dt$t_stat)

```
### coef
```{r, echo=FALSE}

summary(dt$coef)

```
### df
```{r, echo=FALSE}

summary(dt$df)

```

## reproducing part

- Below we will conduct two regression models which present by graph and table, **first we'll use just the previous fifteen studies to reproduce the Card and Krueger's M-A, then we'll implement with our data and we'll see if the study keeps its validity **

- the professor give us the Data of the *Card and krueger's Meta-Ananlysis*, but in that table missing three variable: 
  - information about teenager sub-sample
  
  
  - square root of degree of freedom 
  
  
  - logarithm of square root of degree of freedom
  
  

- we would be able to calculate the last two, but also reading the paper of the others author we couldn't find enough information about teenager sub-sample. 

- In order to control the potential impact of other research characteristics on this dependence, we performed regressions with the logarithm of absolute t-ratio as the dependent variable. Selected independent variables were the logarithm of the square root of degrees of freedom, a number of explanatory variables in given research, and binary variables indicating whether the model specification in given research was logarithmic and if the autoregressive correction was implemented.

## fig 1: what is it, what we can see from it
- now we're going to show our reproduction of the *Card and Krueger* study and then we'll compare their result with the same study implemented with three more studies. 

- we follow step by step their analysis, the first figure shows us the correlation between t_stat and logarithm of square root of degree of freedom.


## reproduced

```{r, warning = F, message = F, echo = F, fig.width = 6, fig.height = 4}
fig_1_old
```

## implemented

```{r, warning = F, message = F, echo = F, fig.width = 6, fig.height = 4}
fig_1_new
```


## something about model
- As we can see the graph shows a negative relationship between the t ratios and the degrees of freedom. The coefficient on the square root of the degrees of freedom is quite far from 1, its theoretical expectation. The inclusion of additional explanatory variables does not change the sign of the coefficient or reduce its effect.

- Next, we will present the reproduced regressions in Table which gives us more information about the correlation. Furthermore, due to the fact the study number 7 is considered as biased data set, therefore we will also conduct table to analysis the result which excludes study number 7.

## reproduced
texting here

```{r, echo = F, results = 'asis', out.width = 6, out.height = 4}
cat(tbl1, sep = '\n')
```

## implemented
texting here

```{r, echo = F, results = 'asis', out.width = 6, out.height = 4}
cat(tbl2, sep = '\n')
```

## is it t-statics always 2?

- One sort of meta-analysis compares a coefficient's strength in each research to its estimated standard error. We employ estimates of the employment effect of a 10% increase in the minimum wage. This approach will provide a positive correlation across studies since the t ratio is defined by b/SE, where b is the estimated employment effect and SE is its standard error.

## Histogram 

```{r, echo=FALSE}
plotNormalHistogram( dt$t_stat, prob = FALSE, col="indianred",
                                             main = "Normal Distribution overlay on Histogram", linecol="black" )
```

## Something about difference in table
texting here

## fig 2: what is it, what we can see from it
texting here

## reproduced

```{r, warning = F, message = F, echo = F, fig.width = 6, fig.height = 4}
fig_2_old
```

## implemented 

```{r, warning = F, message = F, echo = F, fig.width = 6, fig.height = 4}
fig_2_new
```

## result
texting here

## conclusion
Our findings are consistent with the original meta-analysis by Card and Krueger (1995), who argue that these countrintuitive results are caused by the fact that, when the available data are not too large, the norms used by researchers make them were able to find a significant negative correlation between minimum wages and their expected youth employment. However, due to its fragility, the significance decreases as more data is used. Our supplementary paper confirms that this trend in research bias is critical and still exists, with lower t-ratios for papers published after the 15 papers studied by Card and Kruger, even with larger data samples. 

In conclusion, there is strong evidence that research is often biased. Lack of self-awareness and excessive trust in existing literature can be misleading. 

## ended

\centering  Thank You
